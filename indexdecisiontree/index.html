



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="penjelasan tentang k-means clustering & k-nearest nighbord & Decision Tree">
      
      
        <link rel="canonical" href="https://ekoteguh13.github.io/170441100013-dataminning_ekobudiraharjo/indexdecisiontree/">
      
      
        <meta name="author" content="Eko Budi Raharjo">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.jpg">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>decision tree go_trak - k-means clustering & k-nearest nighbord & Decision Tree</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "None", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#decision-tree" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://ekoteguh13.github.io/170441100013-dataminning_ekobudiraharjo/" title="k-means clustering & k-nearest nighbord & Decision Tree" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              k-means clustering & k-nearest nighbord & Decision Tree
            </span>
            <span class="md-header-nav__topic">
              decision tree go_trak
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/ekoteguh13/170441100013-dataminnig_ekobudiraharjo" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    ekoteguh13/170441100013-dataminning_ekobudiraharjo
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

<nav class="md-tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
      
        
      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://ekoteguh13.github.io/170441100013-dataminning_ekobudiraharjo/" title="k-means clustering & k-nearest nighbord & Decision Tree" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    k-means clustering & k-nearest nighbord & Decision Tree
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/ekoteguh13/170441100013-dataminnig_ekobudiraharjo" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    ekoteguh13/170441100013-dataminning_ekobudiraharjo
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../indexkmeans/" title="k-means clustering go_trak" class="md-nav__link">
      k-means clustering go_trak
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../indexknn/" title="k-nearest nighbord go_trak" class="md-nav__link">
      k-nearest nighbord go_trak
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        decision tree go_trak
      </label>
    
    <a href="./" title="decision tree go_trak" class="md-nav__link md-nav__link--active">
      decision tree go_trak
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pengertian-decision-tree" title="pengertian decision tree" class="md-nav__link">
    pengertian decision tree
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pembentukan-pohon-keputusan-terdiri-dari-beberapa-tahap" title="Pembentukan pohon keputusan terdiri dari beberapa tahap :" class="md-nav__link">
    Pembentukan pohon keputusan terdiri dari beberapa tahap :
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pengertian-decision-tree" title="pengertian decision tree" class="md-nav__link">
    pengertian decision tree
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pembentukan-pohon-keputusan-terdiri-dari-beberapa-tahap" title="Pembentukan pohon keputusan terdiri dari beberapa tahap :" class="md-nav__link">
    Pembentukan pohon keputusan terdiri dari beberapa tahap :
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="decision-tree">**DECISION TREE **<a class="headerlink" href="#decision-tree" title="Permanent link">&para;</a></h1>
<h2 id="pengertian-decision-tree">pengertian decision tree<a class="headerlink" href="#pengertian-decision-tree" title="Permanent link">&para;</a></h2>
<p>Decision Tree (Pohon Keputusan) adalah pohon dimana setiap cabangnyamenunjukkan pilihan diantara sejumlah alternatif pilihan yang ada, dan setiapdaunnya menunjukkan keputusan yang dipilih.Decision tree biasa digunakan untuk mendapatkan informasi untuk tujuanpengambilan sebuah keputusan. Decision tree dimulai dengan sebuah root node(titik awal) yang dipakai oleh user untuk mengambil tindakan. Dari node root ini,user memecahnya sesuai dengan algoritma decision tree. Hasil akhirnya adalahsebuah decision tree dengan setiap cabangnya menunjukkan kemungkinansekenario dari keputusan yang diambil serta hasilnya</p>
<h2 id="_1"><img alt="" src="../assets/images/Figure_3_Decision_Tree.jpg" /><a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<h3 id="pembentukan-pohon-keputusan-terdiri-dari-beberapa-tahap"><strong>Pembentukan pohon keputusan terdiri dari beberapa tahap :</strong><a class="headerlink" href="#pembentukan-pohon-keputusan-terdiri-dari-beberapa-tahap" title="Permanent link">&para;</a></h3>
<p><strong>1. Konstruksi pohon</strong> diawali dengan pembentukan akar (terletak paling atas). Kemudian data dibagi berdasarkan atribut-atribut yang cocok untuk dijadikan daun.</p>
<p><strong>2. Pemangkasan pohon (tree pruning)</strong> yaitu mengidentifikasikan dan membuang cabang yang tidak diperlukan pada pohon yang telah terbentuk. Hal ini dikarenakan pohon keputusan yang dikontruksi dapat berukuran besar, maka dapat disederhanakan dengan melakukan pemangkasan berdasarkan nilai kepercayaan (<em>confident level</em>). Pemangkasan pohon dilakukan selain untuk pengurangan ukuran pohon juga bertujuan untuk mengurangi tingkat kesalahan prediksi pada kasus baru dari hasil pemecahan yang dilakukan dengan <em>divide and conquer</em>. <em>Pruning</em> ada dua pendekatan yaitu :</p>
<p><strong>a. Pre-pruning</strong> yaitu menghentikan pembangunan suatu <em>subtree</em> lebih awal (dengan memutuskan untuk tidak lebih jauh mempartisi data training). Saat seketika berhenti, maka <em>node</em> berubah menjadi <em>leaf</em> (node akhir). <em>Node</em> akhir ini menjadi kelas yang paling sering muncul di antara subset sampel.</p>
<p><strong>b. Post-pruning</strong> yaitu menyederhanakan <em>tree</em> dengan cara membuang beberapa cabang <em>subtree*setelah *tree</em> selesai dibangun. <em>Node</em> yang jarang dipotong akan menjadi <em>leaf</em> (node akhir) dengan kelas yang paling sering muncul.</p>
<p><strong>3. Pembentukan aturan keputusan</strong> yaitu membuat aturan keputusan dari pohon yang telah dibentuk. Aturan tersebut dapat dalam bentuk <em>if — then</em> diturunkan dari pohon keputusan dengan melakukan penelusuran dari akar sampai ke daun. Untuk setiap simpul dan percabangannya akan diberikan di <em>if</em>, sedangkan nilai pada daun akan ditulis di <em>then</em>. Setelah semua aturan dibuat maka aturan dapat disederhanakan atau digabung.</p>
<p><em>Decision tree</em> adalah suatu model klasifikasi yang paling populer karena mudah diinterpretasikan oleh manusia. Banyak algoritma yang dapat digunakan dalam pembentukan pohon keputusan seperti ID3, C4.5, CART, dan GUIDE. Algoritma <em>decision tree</em> banyak digunakan dalam proses data mining karena memiliki beberapa <strong>kelebihan</strong> :</p>
<ol>
<li>Mudah mengintegrasikan dengan sistem basis data. </li>
<li>Memiliki ketelitian yang baik. </li>
<li>Dapat menemukan gabungan tak terduga dari suatu data. </li>
<li>Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global dapat diubah menjadi lebih sederhana dan spesifik. </li>
<li>Dapat melakukan eliminasi untuk perhitungan-perhitungan yang tidak diperlukan. Karena ketika menggunakan metode ini maka sampel hanya diuji berdasarkan kriteria atau kelas tertentu. </li>
<li>Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama.</li>
</ol>
<p><strong>Kekurangan</strong> pohon keputusan adalah.</p>
<ol>
<li>Terjadi overlap terutama ketika kelas-kelas dan kriteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan.</li>
<li>Pengakumulasian jumlah error dari setiap tingkat dalam sebuah pohon keputusan yang besar.</li>
<li>Kesulitan dalam mendesain pohon keputusan yang optimal.</li>
<li>Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat bergantung pada bagaimana pohon tersebut didesain.</li>
</ol>
<h1 id="contoh-perhitungan-decision-tree-dengan-algoritma">Contoh Perhitungan Decision Tree dengan Algoritma<a class="headerlink" href="#contoh-perhitungan-decision-tree-dengan-algoritma" title="Permanent link">&para;</a></h1>
<p>Berikut ini akan saya berikan contoh pergitungan decision tree menggunakan algoritma C45 yang saya dapat dari berbagai sumber, sebagai referensi anda.Baik langsung saja berikut pejelasannya</p>
<p>Untuk menentukan bermain tenis atau tidak, kriteria yang diperlukan meliputi:
  -Cuaca
  -Angin
  -Kelembaban
  -Temperatur udara</p>
<p>Salah satu atribut merupakan data solusi per item data yang disebut target atribut -&gt; misalnya atribut “play” degan nilai “main” atau “tidak main”
Atribut memiliki nilai-nilai yang dinamakan “instance”
Misalkan atribut “Cuaca” memiliki instance -&gt; cerah, berawan, dan hujan.  </p>
<p><img alt="" src="../assets/images/1.jpg" /></p>
<p>Berdasakan tabel diatas akan dibuat tabel keputusan untuk menentukan main tenis atau tidak dengan melihat keadaan Outlook (cuaca), Temperatur, Humidity (kelembaban), dan windy (keadaan angin).</p>
<p>Algoritma secara umum:</p>
<p>-Pilih atribut sebagai akar</p>
<p>-Buat cabang untuk tiap2 nilai</p>
<p>-Bagi kasus dalam cabang</p>
<p>-Ulangi proses utk setiap cabang sampai semua kasus pada cabang memiliki kelas yang sama</p>
<p>Memilih atribut berdasarkan nilai “gain” tertinggi dari atribut-atribut yang ada.</p>
<p><strong>Perhitungan Gain</strong></p>
<p><img alt="" src="../assets/images/2.jpg" /></p>
<p>Keterangan:</p>
<p>S : himpunan</p>
<p>A : atribut</p>
<p>n  : jumlah partisi atribut A</p>
<p>| Si | : jumlah kasus pada partisi ke-i</p>
<p>| S |  : jumlah kasus dalam S</p>
<p><strong>Menghitung Nilai Entropy</strong></p>
<p><img alt="" src="../assets/images/3.jpg" /></p>
<p>Perincian algoritma ( langkah 1)</p>
<ul>
<li>Menghitung jumlah kasus seluruhnya, jumlah berkeputusan “Yes” maupun “No”.</li>
<li>Menghitung Entropy dari semua kasus yg terbagi berdasarkan atribut “Outlook”, “Temperature”,“Humidity”, “Windy”.</li>
<li>Lakukan penghitungan Gain utk setiap atributnya</li>
</ul>
<p>Perhitungan</p>
<p><img alt="" src="../assets/images/4.jpg" /></p>
<p>Perhitungan Total Entropy</p>
<p><img alt="" src="../assets/images/5.jpg" /></p>
<p>Menghitung gain pada baris Outlook</p>
<p><img alt="" src="../assets/images/6.jpg" /></p>
<p>Lakukan Hitung Gain untuk temperature, humidity dan windy</p>
<p>Sepert yg terlihat pd tabel, diperoleh bhw atribut dgn Gain tertinggi adalah Humidity -&gt; 0,37</p>
<p>Maka Humidity menjadi node akar</p>
<p>Humidity memiliki dua nilai yaitu “High” dan “Normal”</p>
<p>Humidity -&gt; “Normal” sdh mengklasifikasikan kasus menjadi 1 yaitu keputusannya “yes”</p>
<p>Untuk humidity -&gt; “High” msh perlu dilakukan perhitungn lagi (karena masih terdapat “yes” dan “no”)</p>
<p>Pohon Keputusan Node 1</p>
<p>Perincian Algoritma (Langkah 2)</p>
<p><img alt="" src="../assets/images/7.jpg" /></p>
<p>Hasil perhitungan (Langkah 2)</p>
<p>Didapat Gain tertinggi -&gt; outlook -&gt; 0,69</p>
<p>Maka “Outlook” menjadi node cabang dari atribut humidity yg bernilai “High”</p>
<p>Berdasarkan atribut “Outlook” terdpt 3 nilai </p>
<p>Cloudy</p>
<p>Rainy</p>
<p>Sunny</p>
<p>Krn “Cloudy” pasti bernilai “Yes” dan “Sunny” pasti bernilai “No”, maka tdk perlu dilakukan perhitungan lagi</p>
<p>Sedangkan “Rainy” bernilai “yes” dan “No”, maka masih perlu dilakukan perhitungan lagi</p>
<p>Pohon keputusan node 1.1</p>
<p><img alt="" src="../assets/images/8.jpg" /></p>
<p><img alt="" src="../assets/images/9.jpg" /></p>
<p>Hasil perhitungan (Langkah 3)</p>
<p>Didapat Gain tertinggi -&gt; Windy -&gt; 1</p>
<p>Maka “Windy” menjadi node cabang dari atribut humidity yg bernilai “High” dan outlook yg bernilai “Rainy”</p>
<p>Berdasarkan atribut “Windy” terdpt 2 nilai </p>
<p>True</p>
<p>False</p>
<p>Karena “True” sdh terklasifikasi pasti bernilai “No” dan “False” pasti bernilai “Yes”, maka tidak perlu dilakukan perhitungan lagi</p>
<p>Pohon keputusan node 1.1.2</p>
<p><img alt="" src="../assets/images/10.jpg" /></p>
<p>Hasil perhitungan (Langkah 3)</p>
<p>Berdasarkan node 1.1.2, maka:</p>
<p>“Semua kasus sudah masuk dapat kelas”</p>
<p>Sehingga pohon keputusan diatas merupakan pohon keputusan terakhir yang terbentuk</p>
<p>Seperti yang telah diketahui macam-macam Algoritma Decison tree ada 3 yaitu :</p>
<ul>
<li>Algoritma C4.5</li>
<li>ID3 -&gt; merupakan pengembangan C4.5</li>
<li>CART </li>
</ul>
<p>Perhitungan diatas adalah implementasi menggunakan Algoritma C45, semoga bermanfaat sebagai bahan referensi anda. </p>
<h1 id="implementasi">Implementasi<a href="https://awlya12.github.io/DataMining_AulyaFridayanti_170441100017/customization/#implementasi">¶</a><a class="headerlink" href="#implementasi" title="Permanent link">&para;</a></h1>
<h3 id="library-yang-diperlukan"><strong>Library yang diperlukan</strong><a href="https://awlya12.github.io/DataMining_AulyaFridayanti_170441100017/customization/#library-yang-diperlukan">¶</a><a class="headerlink" href="#library-yang-diperlukan" title="Permanent link">&para;</a></h3>
<ul>
<li>Panda 0.20.3</li>
<li>IPython</li>
<li>Sklearn 0.19.1</li>
<li>pydotplus</li>
<li>graphviz</li>
<li>Jupyter Notebook —-&gt; gua pake ini untuk IDE nya</li>
</ul>
<p>Untuk keseluruhan code nya dapat dilihat di bagian paling bawah tulisan ini</p>
<pre class="codehilite"><code class="language-python">import pandas as pd
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
from IPython.display import Image  
import pydotplus

dataset = pd.read_csv('go_track_tracks2.csv')

x = dataset[['id_android','speed','time','distance','rating','rating_bus','rating_weather']]
y = dataset[['car_or_bus']]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0 )

x_train</code></pre>

<p><img alt="" src="../assets/images/out1.jpg" /></p>
<p><img alt="out2" src="../assets/images/out2.jpg" /></p>
<p><img alt="out3" src="../assets/images/out3.jpg" /></p>
<pre class="codehilite"><code class="language-python">clf = DecisionTreeClassifier(criterion='entropy', max_depth=3)

clf.fit(x_train, y_train)

predicted_y = clf.predict(x_test)</code></pre>

<pre class="codehilite"><code class="language-python">feature_cols = ['id_android','speed','time','distance','rating','rating_bus','rating_weather']
dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True, feature_names = feature_cols,class_names=['negatif','positif'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('PimaIndians.png')
Image(graph.create_png())</code></pre>

<p><img alt="" src="../assets/images/hasil.jpg" /></p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../indexknn/" title="k-nearest nighbord go_trak" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                k-nearest nighbord go_trak
              </span>
            </div>
          </a>
        
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 ekoteguh13
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/ekoteguh13" class="md-footer-social__link fa fa-github-alt"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.8c0d971c.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>